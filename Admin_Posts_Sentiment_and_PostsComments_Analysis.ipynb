{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "By previously investigating the Admin Posts dataset, In the [Interpreting the Relationship between Sentiment and Engagement](https://github.com/Lotass/Natural_Language_Processing/blob/master/User_Engagement_vs_Content_Sentiment.ipynb) notebook. I discovered tha dependency between various independent variable and the Sentiment variable.\n",
=======
    "By previously investigating the Admin Posts dataset, In the [Interpreting the Relationship between Sentiment and Engagement](https://github.com/Lotass/Natural_Language_Processing) notebook. I discovered tha dependency between various independent variable and the Sentiment variable.\n",
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "\n",
    "So, I see that it is kind of __overhead__ to apply __NLP/ Word2vec Model__ on the Admin post text data to fit a model for __Sentiment Analysis__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Approach is as the following, I will build a __Kernel-SVM Model__ based on Independent variables that are highly correlated/ dependent to the dependent variables then I will apply the Feature Extraction technique LDA to boost up the performance of the model.\n",
    "\n",
    "### I will separately use both __sentiment __ and __Comments_Total_Sentiment__ as dependent variables. With keeping the set of the independent variables the same for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "admin_posts = pd.read_excel(\"Hyundai_admin_post_data.xls\")\n",
    "admin_posts = admin_posts.reset_index()\n",
    "admin_posts.drop([\"index\",\"Post_id\",\"user_name\",\"creation_date\",\"creation_time\",\"month\",\"updated_date\",\n",
    "                  \"updated_time\",\"link\",\"picture\",\"user_id\",\"text\",\"type\",\"ontologies\",\"entities\",\"Total_Sentiment\"],\n",
    "                   axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>shares</th>\n",
       "      <th>Comments_Total_Sentiment</th>\n",
       "      <th>Positive_Sentiment</th>\n",
       "      <th>Negative_Sentiment</th>\n",
       "      <th>Number_of_Positive</th>\n",
       "      <th>Number_of_Negative</th>\n",
       "      <th>Number_of_Neutral</th>\n",
       "      <th>Total_Engagement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-2</td>\n",
       "      <td>3</td>\n",
       "      <td>-5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments_count  likes_count  sentiment  shares  Comments_Total_Sentiment  \\\n",
       "0              14          109          2       4                       -14   \n",
       "1              11          121          0       8                        -2   \n",
       "2              14          100          0       4                         7   \n",
       "3               9          178          0      22                         6   \n",
       "4              19          366          0      16                        -1   \n",
       "\n",
       "   Positive_Sentiment  Negative_Sentiment  Number_of_Positive  \\\n",
       "0                   0                 -14                   0   \n",
       "1                   3                  -5                   1   \n",
       "2                   7                   0                   3   \n",
       "3                   6                   0                   3   \n",
       "4                   2                  -3                   1   \n",
       "\n",
       "   Number_of_Negative  Number_of_Neutral  Total_Engagement  \n",
       "0                   2                 10               125  \n",
       "1                   2                  6               138  \n",
       "2                   0                  7               114  \n",
       "3                   0                  5               208  \n",
       "4                   2                 12               397  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = admin_posts.iloc[:,:].values # independent variables\n",
    "y = admin_posts.iloc[:,2].values # sentiment as dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y1 = admin_posts.iloc[:,4].values # Comments_Total_Sentiment as a dependent varable"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 4, 1, 3, -2, -4, 6, 9, -3, 5, -1, 7, 11, 8, -6, -8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values_of_sentiment = admin_posts[\"sentiment\"].unique()\n",
    "list(unique_values_of_sentiment)\n",
    "#print unique_values_of_sentiment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a list of each __unique__ sentiment values and its __frequency__."
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type a list of each unique sentiment values and its frequency."
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 5,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-8: 1,\n",
       " -6: 1,\n",
       " -4: 4,\n",
       " -3: 1,\n",
       " -2: 11,\n",
       " -1: 8,\n",
       " 0: 156,\n",
       " 1: 21,\n",
       " 2: 43,\n",
       " 3: 21,\n",
       " 4: 12,\n",
       " 5: 3,\n",
       " 6: 2,\n",
       " 7: 5,\n",
       " 8: 3,\n",
       " 9: 2,\n",
       " 11: 2}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 5,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((i, list(y).count(i)) for i in y) # for Sentiment"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 6,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-34: 1,\n",
       " -26: 1,\n",
       " -25: 1,\n",
       " -24: 2,\n",
       " -23: 1,\n",
       " -21: 1,\n",
       " -16: 2,\n",
       " -15: 1,\n",
       " -14: 1,\n",
       " -12: 3,\n",
       " -11: 1,\n",
       " -10: 3,\n",
       " -9: 1,\n",
       " -8: 1,\n",
       " -7: 5,\n",
       " -6: 2,\n",
       " -5: 2,\n",
       " -4: 6,\n",
       " -3: 4,\n",
       " -2: 9,\n",
       " -1: 11,\n",
       " 0: 40,\n",
       " 1: 15,\n",
       " 2: 17,\n",
       " 3: 16,\n",
       " 4: 10,\n",
       " 5: 7,\n",
       " 6: 18,\n",
       " 7: 10,\n",
       " 8: 10,\n",
       " 9: 6,\n",
       " 10: 6,\n",
       " 11: 10,\n",
       " 12: 7,\n",
       " 13: 5,\n",
       " 14: 2,\n",
       " 15: 3,\n",
       " 16: 4,\n",
       " 17: 3,\n",
       " 18: 7,\n",
       " 19: 3,\n",
       " 20: 2,\n",
       " 21: 2,\n",
       " 22: 1,\n",
       " 23: 3,\n",
       " 24: 2,\n",
       " 25: 1,\n",
       " 26: 2,\n",
       " 27: 1,\n",
       " 28: 3,\n",
       " 29: 2,\n",
       " 30: 1,\n",
       " 33: 1,\n",
       " 34: 1,\n",
       " 39: 2,\n",
       " 41: 1,\n",
       " 43: 2,\n",
       " 45: 1,\n",
       " 51: 2,\n",
       " 60: 1,\n",
       " 65: 1,\n",
       " 66: 1,\n",
       " 84: 1,\n",
       " 115: 1,\n",
       " 140: 1,\n",
       " 156: 1,\n",
       " 188: 1}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 6,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((i, list(y1).count(i)) for i in y1) # for Comments_Total_Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training and test set with __60%__ Training and __40%__ Testing.\n",
    "** I tried to make the split 70% and 30% for training and testing but the current configuration drived better results **"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 7,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, train_size=0.6,  random_state= 0)\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y1, test_size=0.4, train_size=0.6,  random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to do some __Feature Scaling__ to normalize the training data before applying the __LDA Feature Extraction__ technique."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 8,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eng.Adel\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n",
    "\n",
    "# (Total Comment Sentiment)\n",
    "x_train1 = sc_x.fit_transform(x_train1)\n",
    "x_test1 = sc_x.transform(x_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply linear discriminant analysis __LDA__ to Extract the Features that most discriminate/ seperate the multiple class values of the __Sentiment__ dependent variable."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 9,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eng.Adel\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\Users\\Eng.Adel\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA (n_components = 2)\n",
    "x_train = lda.fit_transform(x_train, y_train)\n",
    "x_test = lda.transform(x_test)\n",
    "\n",
    "# (Total Comment Sentiment)\n",
    "x_train1 = lda.fit_transform(x_train1, y_train1)\n",
    "x_test1 = lda.transform(x_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to optimize the __hyperparameters__ of the __Kernel-SVM__ Algorithm using the __Grid Search__ Technique."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 10,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kernel-SVM Accuracy (sentiment):\n",
      "(0.95798319327731096, {'C': 1, 'gamma': 0.01})\n",
      "The Kernel-SVM Accuracy (Total_Comment_Sentiment):\n",
      "(0.86554621848739499, {'C1': 1, 'gamma1': 0.01})\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "-"
=======
    "from sklearn.svm import SVC \n",
    "\n",
    "# possible hyperparameters\n",
    "c_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "\n",
    "# accuracy\n",
    "best_score = 0\n",
    "best_params = {'C': None, 'gamma': None}\n",
    "best_score1 = 0\n",
    "best_params1 = {'C1': None, 'gamma1': None}\n",
    "\n",
    "# Kernel SVM Hyperparameters optimization (sentiment as dependent variable)\n",
    "for c in c_values:\n",
    "    for gamma in gamma_values:\n",
    "        # train the model for every hyperparameter value pair (sentiment)\n",
    "        classifier = SVC(kernel = 'rbf', C = c, gamma = gamma)\n",
    "        #classifier = SVC(kernel = 'rbf', C = random.randint(0,9), gamma = random.randint(0,3))\n",
    "        classifier.fit(x_train, y_train)\n",
    "        score = classifier.score(x_test, y_test)\n",
    "        \n",
    "        # rate the accuracy of the model using each hyperparameter value pair\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params['C'] =  c\n",
    "            best_params['gamma'] = gamma\n",
    "            \n",
    "# Kernel SVM Hyperparameters optimization (Total_Comment_Sentment as dependent variable)\n",
    "for c1 in c_values:\n",
    "    for gamma1 in gamma_values:\n",
    "        # train the model for every hyperparameter value pair (Total_Comment_Sentiment)\n",
    "        classifier1 = SVC(kernel = 'rbf', C = c1, gamma = gamma1)\n",
    "        classifier1.fit(x_train1, y_train1)\n",
    "        score1 = classifier1.score(x_test1, y_test1)\n",
    "        \n",
    "        if score1 > best_score1:\n",
    "            best_score1 = score1\n",
    "            best_params1['C1'] =  c1\n",
    "            best_params1['gamma1'] = gamma1\n",
    " \n",
    "# best score\n",
    "print \"The Kernel-SVM Accuracy (sentiment):\" \n",
    "print (best_score, best_params) \n",
    "print \"The Kernel-SVM Accuracy (Total_Comment_Sentiment):\"\n",
    "print (best_score1, best_params1) "
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the __Kernel-SVM__ with a __Gaussian Kernel__Nodel to the training data using the derived optimal values of __C__ and __gamma__."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 11,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  0  0  0  0  0  0  8  0  4  1  1  0  1  2  0  0 -4  0  5  0  0  2  0  0\n",
      "  3  0  1  0  8  4  0  8  0  7  6  0  0  1  0 -1  0  2  2  4  0 -2  0  0  0\n",
      "  0  0  0  0  0  0  0  0  2  3  0  0  0  1  3  0  0  0  0 -2  1  0 -2  0  8\n",
      "  0  0  0  0  1  2  0  8 -1  2  7  0  0  0  0 -1  0  0  2  0  2  0  3  1  0\n",
      "  0  2  0  0  0  0  2  3  8 -2  0  0  0  0  0  2  2  3  0] \n",
      "\n",
      "The Predicted Sentiment For Test Data..\n",
      "['Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Positive', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral']\n",
      "\n",
      "\n",
      "[115  11   6   4   9  11   3  29   4 115  13   4   1  21  -1 115   0   7\n",
      "   2  -7   0   4 115   9 115 115  -2   3  11  18   3  12   1  17   0   0\n",
      " 115   2 115 -12  14  -4   9   7   3  -1   8 115  -3   0   4 115  15   6\n",
      "   0  16   2  43 115  -7  -1  -3   3   0   8   3   6 115   1  16  10  -1\n",
      "   3  28  23  -1  18 115   3  24   6   0   6  -2   0   6  11 115  13   7\n",
      "   0   6   0   0   5   0   6   3   7  12 115  11  -4   4   5 115  18   2\n",
      "   1  -2  -4   7  11   3  18  16   0   9   0] \n",
      "\n",
      "The Predicted Sentiment For Test Data..\n",
      "['Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Negative', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "classifier = SVC(kernel = 'rbf', random_state = 0, C = 1, gamma = 0.01)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the test set sentiment values\n",
    "y_pred = classifier.predict(x_test)\n",
    "print y_pred, \"\\n\"\n",
    "\n",
    "print \"The Predicted Sentiment For Test Data..\"\n",
    "y_pred_text = []\n",
    "for pred in y_pred:\n",
    "    if pred == 0:\n",
    "        y_pred_text.append(\"Neutral\")\n",
    "    elif pred > 0:\n",
    "        y_pred_text.append(\"Positive\")\n",
    "    elif pred < 0:\n",
    "        y_pred_text.append(\"Negative\")\n",
    "\n",
    "print y_pred_text \n",
    "\n",
    "print \"\\n\"\n",
    "\n",
    "classifier.fit(x_train1, y_train1)\n",
    "\n",
    "# Predicting the test set Total_Comment_Sentiment values\n",
    "y_pred1 = classifier.predict(x_test1)\n",
    "print y_pred1, \"\\n\"\n",
    "\n",
    "print \"The Predicted Sentiment For Test Data..\"\n",
    "y_pred_text1 = []\n",
    "for pred in y_pred1:\n",
    "    if pred == 0:\n",
    "        y_pred_text1.append(\"Neutral\")\n",
    "    elif pred > 0:\n",
    "        y_pred_text1.append(\"Positive\")\n",
    "    elif pred < 0:\n",
    "        y_pred_text1.append(\"Negative\")\n",
    "\n",
    "print y_pred_text1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the __Confusion matrix__ of the __Kernel-SVM__  Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 12,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 69  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 14  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  2  0  0]]\n",
      "\n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm1 = confusion_matrix(y_test1, y_pred1)\n",
    "print cm \n",
    "print \"\\n\"\n",
    "print cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By interpreting the __Confusion matrix__ where __sentiment__ is the dependent variable, the diagonal which represents the __True Positive (TP)__ = __114__ .\n",
    "\n",
    "By interpreting the __Confusion matrix__ where __Total_Comment_Sentiment__  is the dependent variable -- One hell of a CM BTW!- \n",
    "   <img src= \"Capture.PNG\">\n",
    "   \n",
    "#####  It is ___45*45___  Matrix with actually a pretty good results.\n",
    "\n",
    "\n",
    "#### [Calculate the precision, recall Using Confusion Matrix of a Multiclass Classification Problem](http://text-analytics101.rxnlp.com/2014/10/computing-precision-and-recall-for.html) \n",
    "Or using this [paper](http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf)\n",
    "- True positive: diagonal position, cm(x, x).\n",
    "- False positiv__Confusion matrix__: sum of column x (without main diagonal), sum(cm(:, x))-cm(x, x).\n",
    "- False negative: sum of row x (without main diagonal), sum(cm(x, :), 2)-cm(x, x). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the __Kernel-SVM__  Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 13,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kernel-SVM Accuracy (sentiment)= 95.798319 %\n",
      "The Kernel-SVM Accuracy (Total_Comment_Sentiment)= 86.554622 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)*100   \n",
    "acc1 = accuracy_score(y_test1, y_pred1)*100  \n",
    "print \"The Kernel-SVM Accuracy (sentiment)= %f\" % acc, \"%\"\n",
    "print \"The Kernel-SVM Accuracy (Total_Comment_Sentiment)= %f\" % acc1, \"%\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note::\n",
    "Although both __sentiment__ and __Comments_Total_Sentiment__ seems to be dependent on the the same independent varables, they are __not depndent__ on each other as we can see by calculating the __Pearson correlaton factor__ between them it turns out that it is significantly low ~ = __- 0.0121__"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 14,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pearson correlaton factor between sentiment and Comments_Total_Sentiment \n",
      "(-0.012155185373358458, 0.83503489629502392)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "x2 = admin_posts[\"sentiment\"]\n",
    "y2 = admin_posts[\"Comments_Total_Sentiment\"]\n",
    "\n",
    "print \" Pearson correlaton factor between sentiment and Comments_Total_Sentiment \"\n",
    "print (scipy.stats.pearsonr(x2, y2))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
