{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Arabic Text dataset, this text is cleaned from all (Franko-Arab) and English Text. As both of English and Franko-Arab don't add much weight or strength  to my model,  I think they more like noise in this dataset.\n",
    "\n",
    "So, I decided to get rid of them and only work with arabic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ياجماعة البيع بسعر الجملة كاوتش سيارات جميع ال...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هو مش العروض دي طول شهر رمضان ازاااي التوكيل ي...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لوسمحتو سعر الفوليستير كام</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H IX35  2000 بكام واقرب مكان الى ملوى للشراء</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>مرسى جدا لللآهتمام</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  ┘è╪د╪ش┘à╪د╪╣╪ر ╪د┘╪ذ┘è╪╣ ╪ذ╪│╪╣╪▒ ╪د┘╪ش┘à┘╪ر ┘â╪د┘ê╪ز╪┤ ╪│┘è╪د╪▒╪د╪ز ╪ش┘à┘è╪╣ ╪د┘...        0.0\n",
       "1  ┘ç┘ê ┘à╪┤ ╪د┘╪╣╪▒┘ê╪╢ ╪»┘è ╪╖┘ê┘ ╪┤┘ç╪▒ ╪▒┘à╪╢╪د┘ ╪د╪▓╪د╪د╪د┘è ╪د┘╪ز┘ê┘â┘è┘ ┘è...        0.0\n",
       "2                         ┘┘ê╪│┘à╪ص╪ز┘ê ╪│╪╣╪▒ ╪د┘┘┘ê┘┘è╪│╪ز┘è╪▒ ┘â╪د┘à        0.0\n",
       "3       H IX35  2000 ╪ذ┘â╪د┘à ┘ê╪د┘é╪▒╪ذ ┘à┘â╪د┘ ╪د┘┘ë ┘à┘┘ê┘ë ┘┘╪┤╪▒╪د╪ة        0.0\n",
       "5                                 ┘à╪▒╪│┘ë ╪ش╪»╪د ┘┘┘╪ت┘ç╪ز┘à╪د┘à        0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_text = pd.read_excel(\"all_text2.xlsx\")\n",
    "arabic_text_list = arabic_text[\"text\"].values.astype(str).tolist()\n",
    "\n",
    "# Arabic Stop Words set\n",
    "arbic_stop_words = pd.read_csv('stopwords-ar.txt', sep=\" \", header=None)\n",
    "arbic_stop_words_set = arbic_stop_words.values.astype(str).tolist()\n",
    "\n",
    "arabic_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to remove empty lists from a list of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word2Vec Model__ expects single sentences, each one as a list of words. In other words, the input format is a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_empty(l):\n",
    "    return tuple(filter(lambda x:not isinstance(x, (str, list, tuple)) or x,\n",
    "           (remove_empty(x) if isinstance(x, (tuple, list)) else x for x in l)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Clean the training set Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_arabictext_train = []\n",
    "for i in range(int(len(arabic_text_list)-len(arabic_text_list)*0.4)): # train on 60% of the Corpus\n",
    "    #posttext = txt.decode('utf-8')\n",
    "    match = re.sub(r\"http\\S+\",\" \", arabic_text_list[i])  # remove http liks\n",
    "    match = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \" \", match) # remove hashtags\n",
    "    match = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \" \", match) # remove numbers\n",
    "    match = re.sub(r\"(?:[a-z][a-z'\\-_]+[a-z])\", \" \", match) # remove words with - and '\n",
    "    match = re.sub(r'(?:@[\\w_]+)', \" \", match)# remove @-mentions\n",
    "    emotions_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "    match = re.sub(emotions_str, \" \", match)\n",
    "    match = match.replace('\"', \" \") # remove double quotes\n",
    "    #words = [w for w in match if not w in arbic_stop_words_set] # remove arabic stop words\n",
    "    match = match.lower().split() # tokenize to words\n",
    "    match = [x for x in match if x != []] # remove \"falsy\", e.g. empty strings, empty tuples, zeros, empty lists\n",
    "    match = remove_empty(match)\n",
    "    cleaned_arabictext_train.append(match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Clean the test set Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cleaned_arabictext_test = []\n",
    "for i in range(int(len(arabic_text_list)-len(arabic_text_list)*0.6)): # test on 40% of the Corpus\n",
    "    #posttext = txt.decode('utf-8')\n",
    "    match = re.sub(r\"http\\S+\",\" \", arabic_text_list[i])  # remove http liks\n",
    "    match = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \" \", match) # remove hashtags\n",
    "    match = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \" \", match) # remove numbers\n",
    "    match = re.sub(r\"(?:[a-z][a-z'\\-_]+[a-z])\", \" \", match) # remove words with - and '\n",
    "    match = re.sub(r'(?:@[\\w_]+)', \" \", match)# remove @-mentions\n",
    "    emotions_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "    match = re.sub(emotions_str, \" \", match)\n",
    "    match = match.replace('\"', \" \") # remove double quotes\n",
    "    #words = [w for w in match if not w in arbic_stop_words_set] # remove arabic stop words\n",
    "    match = match.lower().split() # tokenize to words\n",
    "    match = [x for x in match if x != []] # remove \"falsy\", e.g. empty strings, empty tuples, zeros, empty lists\n",
    "    cleaned_arabictext_test.append(match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it\n",
    "# so that Word2vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\\n\"\n",
    "model = word2vec.Word2Vec(cleaned_arabictext_train, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"Arabic_Comment_Posts_Word2vec_Model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "\n",
    "One challenge with the dataset is the variable-length text. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review. Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review.\n",
    "\n",
    "One method is to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    A function to average all of the word vectors\n",
    "    in a given paragraph\n",
    "    \"\"\"\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the\n",
    "    # words in the model's vocabulary. convert it to a set\n",
    "    # for speed,,\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # loop over each word in the review and, if it is \n",
    "    # in the model's vocabulary.\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(text, model, num_features):\n",
    "    \"\"\"\n",
    "    Given a set of text (each one a list of words),\n",
    "    caculate the average feature vector for each one \n",
    "    and return a 2D numpy array\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    # Prellocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(text), num_features), dtype = \"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in text:\n",
    "        # print a status msg every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print \"text %d of %d\" % (counter, len(text))\n",
    "        # call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets, using the functions we defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Creating average feature vecs for training reviews\"\n",
    "trainDataVecs = getAvgFeatureVecs(cleaned_arabictext_train, model, num_features)\n",
    "\n",
    "print \"Creating average feature vecs for training reviews\"\n",
    "testDataVecs = getAvgFeatureVecs(cleaned_arabictext_test, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the train and text datavecs it turned out that they include a lot of nans.....!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataVecs_frame = pd.DataFrame(trainDataVecs)\n",
    "testDataVecs_frame = pd.DataFrame(testDataVecs)\n",
    "trainDataVecs_frame.dropna(inplace=True)\n",
    "testDataVecs_frame.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a random forest to the training data, using 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(trainDataVecs_frame, arabic_text[\"sentiment\"][0:2781] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict(testDataVecs_frame)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(arabic_text[], result)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"text\":cleaned_arabictext_test[:1855], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Predicted Sentiment is loaded in a the [Word2Vec_AverageVectors.csv](https://docs.google.com/spreadsheets/d/1g6OE74YUVOljlWQyFPDu6LCu3527V0RbZ1aqTBBEKEI/edit?usp=sharing) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
