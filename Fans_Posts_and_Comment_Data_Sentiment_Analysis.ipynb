{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
<<<<<<< HEAD
   "source": [
    "### I am here building a (__Word2Vec Model__) using the text data of the Hyundai Fans Posts and Comments data.\n"
   ]
=======
   "source": []
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment_data = pd.read_excel(\"Hyundai_comment_data.xls\")\n",
    "fans_posts = pd.read_excel(\"Hyundai_fans_post_data.xls\")\n",
    "\n",
    "# drop irrelevant columns\n",
    "comment_data.drop([\"Comment_id\",\"user_name\",\"creation_date\",\"creation_time\",\"month\",\"weekday\",\"hour\",\n",
    "                                  \"user_id\",\"Post_id\"], axis=1, inplace=True)\n",
    "fans_posts.drop([\"Post_id\",\"user_name\",\"creation_date\",\"creation_time\",\"month\",\"weekday\",\"hour\",\"updated_date\",\n",
    "                              \"updated_time\",\"link\",\"picture\",\"user_id\"], axis=1, inplace=True)\n",
    "\n",
    "# drop columns Which are all NANs \n",
    "comment_data.dropna(axis=1, how='all')\n",
    "fans_posts.dropna(axis=1, how='all')\n",
    "\n",
    "# gather text corpus \n",
    "comment_text = comment_data[[\"text\",\"sentiment\"]]\n",
    "fans_text = fans_posts[[\"text\",\"sentiment\"]]\n",
    "\n",
    "pieces = (comment_text,fans_text)\n",
    "for sub_df in pieces:\n",
    "    sub_df.columns = [[\"text\",\"sentiment\"]]\n",
    "all_text = pd.concat(pieces, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am import __Arabic Text__ dataset, this text is cleaned from all _(Franko-Arab)_ and _English Text_. As both of English and Franko-Arab don't add much weight or strength  to my model,  I think they more like noise in this dataset.\n",
    "\n",
    "So, I decided to get rid of them and only work with  __Arabic Text__. Also __Arabic Text__ represents the majority of the Posts/ Comments.\n"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Arabic Text dataset, this text is cleaned from all (Franko-Arab) and English Text. As both of English and Franko-Arab don't add much weight or strength  to my model,  I think they more like noise in this dataset.\n",
    "\n",
    "So, I decided to get rid of them and only work with arabic text"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 2,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ياجماعة البيع بسعر الجملة كاوتش سيارات جميع ال...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>هو مش العروض دي طول شهر رمضان ازاااي التوكيل ي...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لوسمحتو سعر الفوليستير كام</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H IX35  2000 بكام واقرب مكان الى ملوى للشراء</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>مرسى جدا لللآهتمام</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
<<<<<<< HEAD
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>عرض العيد بكام سعر الفيرنا٢٠١٥</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ياترى الفيرنا الشكل الجديد 2016اللى يختلف تمام...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ممكن سعر الفئه الثالثه اى اكس 35 بلص</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>هو خصم 6000 بتاع شهر رمضان لسه شغال عايز النتر...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>هو انا ممكن اعرف اسعار العربيه الفيرنا عامله ك...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
=======
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
<<<<<<< HEAD
       "                                                 text  sentiment\n",
       "0   ┘è╪د╪ش┘à╪د╪╣╪ر ╪د┘╪ذ┘è╪╣ ╪ذ╪│╪╣╪▒ ╪د┘╪ش┘à┘╪ر ┘â╪د┘ê╪ز╪┤ ╪│┘è╪د╪▒╪د╪ز ╪ش┘à┘è╪╣ ╪د┘...        0.0\n",
       "1   ┘ç┘ê ┘à╪┤ ╪د┘╪╣╪▒┘ê╪╢ ╪»┘è ╪╖┘ê┘ ╪┤┘ç╪▒ ╪▒┘à╪╢╪د┘ ╪د╪▓╪د╪د╪د┘è ╪د┘╪ز┘ê┘â┘è┘ ┘è...        0.0\n",
       "2                          ┘┘ê╪│┘à╪ص╪ز┘ê ╪│╪╣╪▒ ╪د┘┘┘ê┘┘è╪│╪ز┘è╪▒ ┘â╪د┘à        0.0\n",
       "3        H IX35  2000 ╪ذ┘â╪د┘à ┘ê╪د┘é╪▒╪ذ ┘à┘â╪د┘ ╪د┘┘ë ┘à┘┘ê┘ë ┘┘╪┤╪▒╪د╪ة        0.0\n",
       "5                                  ┘à╪▒╪│┘ë ╪ش╪»╪د ┘┘┘╪ت┘ç╪ز┘à╪د┘à        0.0\n",
       "6                      ╪╣╪▒╪╢ ╪د┘╪╣┘è╪» ╪ذ┘â╪د┘à ╪│╪╣╪▒ ╪د┘┘┘è╪▒┘╪د┘ت┘ب┘ة┘ح        0.0\n",
       "7   ┘è╪د╪ز╪▒┘ë ╪د┘┘┘è╪▒┘╪د ╪د┘╪┤┘â┘ ╪د┘╪ش╪»┘è╪» 2016╪د┘┘┘ë ┘è╪«╪ز┘┘ ╪ز┘à╪د┘à...        2.0\n",
       "8                ┘à┘à┘â┘ ╪│╪╣╪▒ ╪د┘┘╪خ┘ç ╪د┘╪س╪د┘╪س┘ç ╪د┘ë ╪د┘â╪│ 35 ╪ذ┘╪╡        0.0\n",
       "9   ┘ç┘ê ╪«╪╡┘à 6000 ╪ذ╪ز╪د╪╣ ╪┤┘ç╪▒ ╪▒┘à╪╢╪د┘ ┘╪│┘ç ╪┤╪║╪د┘ ╪╣╪د┘è╪▓ ╪د┘┘╪ز╪▒...        0.0\n",
       "10  ┘ç┘ê ╪د┘╪د ┘à┘à┘â┘ ╪د╪╣╪▒┘ ╪د╪│╪╣╪د╪▒ ╪د┘╪╣╪▒╪ذ┘è┘ç ╪د┘┘┘è╪▒┘╪د ╪╣╪د┘à┘┘ç ┘â...        0.0"
      ]
     },
     "execution_count": 25,
=======
       "                                                text  sentiment\n",
       "0  ┘è╪د╪ش┘à╪د╪╣╪ر ╪د┘╪ذ┘è╪╣ ╪ذ╪│╪╣╪▒ ╪د┘╪ش┘à┘╪ر ┘â╪د┘ê╪ز╪┤ ╪│┘è╪د╪▒╪د╪ز ╪ش┘à┘è╪╣ ╪د┘...        0.0\n",
       "1  ┘ç┘ê ┘à╪┤ ╪د┘╪╣╪▒┘ê╪╢ ╪»┘è ╪╖┘ê┘ ╪┤┘ç╪▒ ╪▒┘à╪╢╪د┘ ╪د╪▓╪د╪د╪د┘è ╪د┘╪ز┘ê┘â┘è┘ ┘è...        0.0\n",
       "2                         ┘┘ê╪│┘à╪ص╪ز┘ê ╪│╪╣╪▒ ╪د┘┘┘ê┘┘è╪│╪ز┘è╪▒ ┘â╪د┘à        0.0\n",
       "3       H IX35  2000 ╪ذ┘â╪د┘à ┘ê╪د┘é╪▒╪ذ ┘à┘â╪د┘ ╪د┘┘ë ┘à┘┘ê┘ë ┘┘╪┤╪▒╪د╪ة        0.0\n",
       "5                                 ┘à╪▒╪│┘ë ╪ش╪»╪د ┘┘┘╪ت┘ç╪ز┘à╪د┘à        0.0"
      ]
     },
     "execution_count": 2,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_text = pd.read_excel(\"all_text2.xlsx\")\n",
    "arabic_text_list = arabic_text[\"text\"].values.astype(str).tolist()\n",
<<<<<<< HEAD
    "arabic_text_sentiment_list = arabic_text[\"sentiment\"].values.astype(int).tolist()\n",
=======
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "\n",
    "# Arabic Stop Words set\n",
    "arbic_stop_words = pd.read_csv('stopwords-ar.txt', sep=\" \", header=None)\n",
    "arbic_stop_words_set = arbic_stop_words.values.astype(str).tolist()\n",
    "\n",
<<<<<<< HEAD
    "arabic_text.head(10)"
=======
    "arabic_text.head()"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "A function to remove empty lists from a list of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "__Word2Vec Model__ expects single sentences, each one as a list of words. In other words, the input format is a list of lists."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_empty(l):\n",
<<<<<<< HEAD
    "    \"\"\"\n",
    "    A function to remove empty lists\n",
    "    from a list of list\n",
    "    \"\"\"\n",
    "    return tuple(filter(lambda x:not isinstance(x, (str, list, tuple)) or x,\n",
    "           (remove_empty(x) if isinstance(x, (tuple, list)) else x for x in l)))"
=======
    "    return tuple(filter(lambda x:not isinstance(x, (str, list, tuple)) or x,\n",
    "           (remove_empty(x) if isinstance(x, (tuple, list)) else x for x in l)))\n",
    "\n"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### I will Split My Text Corpus with percentage 60% for Training and 40% for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Clean The training set Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the sentiment column (dependent variable)\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train, y_test = train_test_split(arabic_text[\"sentiment\"], test_size = 0.4)"
=======
    "First, Clean the training set Corpus"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 4,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_arabictext_train = []\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "for i in range(int(len(arabic_text_list)-len(arabic_text_list)*0.4)): # train on 60% of the Corpus\n",
    "    #posttext = txt.decode('utf-8')\n",
    "    match = re.sub(r\"http\\S+\",\" \", arabic_text_list[i])  # remove http liks\n",
    "    match = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \" \", match) # remove hashtags\n",
    "    match = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \" \", match) # remove numbers\n",
    "    match = re.sub(r\"(?:[a-z][a-z'\\-_]+[a-z])\", \" \", match) # remove words with - and '\n",
    "    match = re.sub(r'(?:@[\\w_]+)', \" \", match)# remove @-mentions\n",
    "    emotions_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "    match = re.sub(emotions_str, \" \", match)\n",
    "    match = match.replace('\"', \" \") # remove double quotes\n",
    "    #words = [w for w in match if not w in arbic_stop_words_set] # remove arabic stop words\n",
    "    match = match.lower().split() # tokenize to words\n",
    "    match = [x for x in match if x != []] # remove \"falsy\", e.g. empty strings, empty tuples, zeros, empty lists\n",
    "    match = remove_empty(match)\n",
    "    cleaned_arabictext_train.append(match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Then, Clean the test set Corpus"
=======
    "Then, Clean the test set Corpus"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 5,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "cleaned_arabictext_test = []\n",
    "\n",
=======
    "\n",
    "cleaned_arabictext_test = []\n",
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "for i in range(int(len(arabic_text_list)-len(arabic_text_list)*0.6)): # test on 40% of the Corpus\n",
    "    #posttext = txt.decode('utf-8')\n",
    "    match = re.sub(r\"http\\S+\",\" \", arabic_text_list[i])  # remove http liks\n",
    "    match = re.sub(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", \" \", match) # remove hashtags\n",
    "    match = re.sub(r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \" \", match) # remove numbers\n",
    "    match = re.sub(r\"(?:[a-z][a-z'\\-_]+[a-z])\", \" \", match) # remove words with - and '\n",
    "    match = re.sub(r'(?:@[\\w_]+)', \" \", match)# remove @-mentions\n",
    "    emotions_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "    match = re.sub(emotions_str, \" \", match)\n",
    "    match = match.replace('\"', \" \") # remove double quotes\n",
    "    #words = [w for w in match if not w in arbic_stop_words_set] # remove arabic stop words\n",
    "    match = match.lower().split() # tokenize to words\n",
    "    match = [x for x in match if x != []] # remove \"falsy\", e.g. empty strings, empty tuples, zeros, empty lists\n",
<<<<<<< HEAD
    "    match = remove_empty(match)\n",
=======
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "    cleaned_arabictext_test.append(match)\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Start fitting the training data to build the Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
=======
   "cell_type": "code",
   "execution_count": 6,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it\n",
    "# so that Word2vec creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\\n\"\n",
    "model = word2vec.Word2Vec(cleaned_arabictext_train, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"Arabic_Comment_Posts_Word2vec_Model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## From Words To Paragraphs, Attempt: Vector Averaging\n",
    "\n",
    "One challenge with the dataset is the variable-length text. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review. Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review.\n",
    "\n",
    "- One method is to simply __average the word vectors__ in a given post/ comment."
=======
    "## From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "\n",
    "One challenge with the dataset is the variable-length text. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review. Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review.\n",
    "\n",
    "One method is to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise)."
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 7,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    A function to average all of the word vectors\n",
    "    in a given paragraph\n",
    "    \"\"\"\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the\n",
    "    # words in the model's vocabulary. convert it to a set\n",
    "    # for speed,,\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    # loop over each word in the review and, if it is \n",
    "    # in the model's vocabulary.\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 8,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(text, model, num_features):\n",
    "    \"\"\"\n",
    "    Given a set of text (each one a list of words),\n",
    "    caculate the average feature vector for each one \n",
    "    and return a 2D numpy array\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    # Prellocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(text), num_features), dtype = \"float32\")\n",
    "    # Loop through the reviews\n",
    "    for review in text:\n",
    "        # print a status msg every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print \"text %d of %d\" % (counter, len(text))\n",
    "        # call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        \n",
    "        counter = counter + 1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Calculate average feature vectors for training and testing sets, using the functions we defined above. "
=======
    "Calculate average feature vectors for training and testing sets, using the functions we defined above. "
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 9,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Creating average feature vecs for training reviews\"\n",
    "trainDataVecs = getAvgFeatureVecs(cleaned_arabictext_train, model, num_features)\n",
    "\n",
    "print \"Creating average feature vecs for training reviews\"\n",
    "testDataVecs = getAvgFeatureVecs(cleaned_arabictext_test, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "By observing the train and text data vectors it turned out that they include some NAN Rows.....!\n",
    "So, I will convert the __Train and Test DataVecs__ to Pandas DataFrame. then drop the NAN rows."
=======
    "By observing the train and text datavecs it turned out that they include a lot of nans.....!\n"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 10,
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDataVecs_frame = pd.DataFrame(trainDataVecs)\n",
    "testDataVecs_frame = pd.DataFrame(testDataVecs)\n",
    "trainDataVecs_frame.dropna(inplace=True)\n",
    "testDataVecs_frame.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Fit a random forest to the training data, using 100 trees"
=======
    "Fit a random forest to the training data, using 100 trees"
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 11,
   "metadata": {
    "collapsed": true
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
<<<<<<< HEAD
    "forest = forest.fit(trainDataVecs_frame, arabic_text[\"sentiment\"][0:2781] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test & extract results \n",
    "result = forest.predict(testDataVecs_frame)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(arabic_text_sentiment_list[-1855:], result)\n",
=======
    "forest = forest.fit(trainDataVecs_frame, arabic_text[\"sentiment\"][0:2781] )\n",
    "\n",
    "# Test & extract results \n",
    "result = forest.predict(testDataVecs_frame)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(arabic_text[], result)\n",
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame( data={\"text\":cleaned_arabictext_test[:1855], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Predicted Sentiment is loaded in a the [Word2Vec_AverageVectors.csv](https://docs.google.com/spreadsheets/d/1g6OE74YUVOljlWQyFPDu6LCu3527V0RbZ1aqTBBEKEI/edit?usp=sharing) file."
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "To be honest I don't exactly know how good is this model. I am not sure if it has a high permormance.\n",
    "\n",
    "And that because of the _NAN values_ that appears in the Test/Train data Vectors. The Training/ Testing data size mismatch the Training/ Testing data vectors size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Capture1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a __36*36 Confusion Matrix__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(arabic_text_sentiment_list[-1855:], result)*100   \n",
    "print \"The Accuracy= %f\" % acc, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.79245283018868"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __A Not Very Good Model Performance To be Honest!__"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
=======
  }
 ],
 "metadata": {
>>>>>>> a1f2661148e07508a1e66a8bd747caa62413a9da
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
